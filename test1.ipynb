{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PretrainedConfig, BertTokenizer, BertModel, BertForSequenceClassification\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "label_list = [\"anger\", \"fear\", \"joy\", \"sadness\", \"surprise\"]\n",
    "\n",
    "def fine_tuning(text, labels):\n",
    "    model.train()\n",
    "    # Tokenize input\n",
    "    indexed_tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor(indexed_tokens).unsqueeze(0)\n",
    "    # 相当于 tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "    # labels\n",
    "    labels = torch.tensor([labels])  # Batch size 1\n",
    "\n",
    "    # If you have a GPU, put everything on cuda\n",
    "    tokens_tensor = tokens_tensor.to('cuda')\n",
    "    labels = labels.to('cuda')\n",
    "\n",
    "    #\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "        print(\"loss: \", loss)\n",
    "        print(\"logits: \", logits)\n",
    "\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# model\n",
    "config = {\n",
    "    \"attention_probs_dropout_prob\": 0.1,\n",
    "    \"finetuning_task\": None,\n",
    "    \"hidden_act\": \"gelu\",\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"hidden_size\": 768,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 3072,\n",
    "    \"layer_norm_eps\": 1e-12,\n",
    "    \"max_position_embeddings\": 512,\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"num_hidden_layers\": 12,\n",
    "    \"num_labels\": 5,\n",
    "    \"output_attentions\": False,\n",
    "    \"output_hidden_states\": False,\n",
    "    \"pruned_heads\": {},\n",
    "    \"torchscript\": False,\n",
    "    \"type_vocab_size\": 2,\n",
    "    \"use_bfloat16\": False,\n",
    "    \"vocab_size\": 30522\n",
    "}\n",
    "pretrainedConfig = PretrainedConfig.from_dict(config)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', config=pretrainedConfig)\n",
    "# model.eval()  # Sets the module in evaluation mode(评估模式).\n",
    "# To train the model, you should first set it back in training mode with model.train()\n",
    "# If you have a GPU, put everything on cuda\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuning(text, labels):\n",
    "    model.train()\n",
    "    # Tokenize input\n",
    "    indexed_tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor(indexed_tokens).unsqueeze(0)\n",
    "    # 相当于 tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "    # labels\n",
    "    labels = torch.tensor([labels])  # Batch size 1\n",
    "\n",
    "    # If you have a GPU, put everything on cuda\n",
    "    tokens_tensor = tokens_tensor.to('cuda')\n",
    "    labels = labels.to('cuda')\n",
    "\n",
    "    #\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "        print(\"loss: \", loss)\n",
    "        print(\"logits: \", logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_seq = \"You shouldn't try to cover it up.\"\n",
    "test_labels = [0, 0, 0, 0, 0]\n",
    "fine_tuning(test_seq, test_labels)\n",
    "#\n",
    "# with open(r\"./data/trainset.txt\") as f:\n",
    "#     for line in f:\n",
    "#         l = line.rstrip().split('##')\n",
    "#         fine_tuning(l[0], l[1])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fine_tuning(batch,max_length=512):\n",
    "    model.train()\n",
    "    tokens_tensors = []\n",
    "    attention_masks=[]\n",
    "    labels_tensors = []\n",
    "\n",
    "    for text,labels in batch:\n",
    "        # Tokenize input\n",
    "        indexed_tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        seq_length=len(indexed_tokens)\n",
    "        padding_length=max_length-seq_length    # 总输入长度 - 序列化的长度\n",
    "\n",
    "        tokens_tensor=indexed_tokens+[0]*padding_length\n",
    "        tokens_tensors.append(tokens_tensor)\n",
    "\n",
    "        attention_mask=[1]*seq_length+[0]*padding_length\n",
    "\n",
    "        labels_tensors.append(labels)\n",
    "\n",
    "    # If you have a GPU, put everything on cuda\n",
    "    tokens_tensors = torch.tensor(tokens_tensors).to('cuda')\n",
    "    labels_tensors = torch.tensor(labels_tensors).to('cuda')\n",
    "    # print(tokens_tensors)\n",
    "    print(labels_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = \"You let them take you, and that gave me enough time to escape.\"\n",
    "test_label = [0, 0, 0, 0, 0]\n",
    "test_seq1 = \"We could just not go. - That's a bad idea. - We're already under attack.\"\n",
    "test_label1 = [1, 1, 0, 0, 0]\n",
    "batch = [(test_seq, test_label), (test_seq1, test_label1)]\n",
    "fine_tuning(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}